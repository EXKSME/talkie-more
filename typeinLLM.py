import sounddevice as sd
import numpy as np
import pyautogui
import pyperclip
import requests
from funasr import AutoModel
from queue import Queue
import time
import threading
import re
import json
from difflib import SequenceMatcher

# =========================
# å‚æ•°åŒºï¼ˆä½ åé¢è°ƒå‚å°±è°ƒè¿™é‡Œï¼‰
# =========================
SILENCE_TIMEOUT = 0.6          # é™éŸ³è¶…è¿‡å¤šå°‘ç§’ -> commit
ENERGY_THRESHOLD = 0.008       # é™éŸ³èƒ½é‡é˜ˆå€¼ï¼ˆä¸åŒéº¦å…‹é£è¦è°ƒï¼Œåå°æ›´æ•æ„Ÿï¼‰
MIN_COMMIT_GAP = 0.8           # ä¸¤æ¬¡ commit æœ€å°é—´éš”ï¼ˆé˜²æŠ–ï¼‰

OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen3:1.7b"

LLM_MODE = "smart_markdown"    # "clean" / "markdown" / "smart_markdown"

# è¾“å‡ºå®‰å…¨é—¸é—¨é˜ˆå€¼ï¼ˆå»ºè®®å…ˆç”¨è¿™ç»„ï¼Œåé¢å†å¾®è°ƒï¼‰
SAFE_SIM_HIGH = 0.70
SAFE_SIM_LOW  = 0.55
SAFE_NGRAM_COV = 0.55

# ç²˜è´´èŠ‚å¥
pyautogui.PAUSE = 0.005


# =========================
# å·¥å…·ï¼šdiff æ–°å¢æ–‡æœ¬
# =========================
def diff_new_part(prev: str, curr: str) -> str:
    i = 0
    while i < len(prev) and i < len(curr) and prev[i] == curr[i]:
        i += 1
    return curr[i:]


# =========================
# å·¥å…·ï¼špasteï¼ˆæ ¸å¿ƒï¼‰
# =========================
def paste_text(text: str):
    if not text:
        return
    pyperclip.copy(text)
    pyautogui.hotkey("command", "v")


# =========================
# å·¥å…·ï¼šå›åˆ  N ä¸ªå­—ç¬¦ï¼ˆç”¨äºæ›¿æ¢ previewï¼‰
# =========================
def delete_chars(n: int):
    if n <= 0:
        return
    pyautogui.press("backspace", presses=n, interval=0)


# =========================
# Step1ï¼šå·¥ç¨‹é¢„æ¸…æ´—ï¼ˆç»“æ„åŒ–å‰æ°å¼€ç²˜è¿ï¼‰
# =========================
_ORD_PATTERN = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ä¸ª\s*[:ï¼š])'

def split_ordered_items(text: str) -> str:
    """æŠŠ 'ç¬¬ä¸€ä¸ªï¼šxxç¬¬äºŒä¸ªï¼šyy' æ‹†æˆå¤šè¡Œï¼ˆä»…æ‹†ç»“æ„ï¼Œä¸æ–°å¢å†…å®¹ï¼‰"""
    if not text:
        return text
    if text.count("ç¬¬") < 2:
        return text
    if not (("ç¬¬äºŒä¸ª" in text) or ("ç¬¬ä¸‰ä¸ª" in text) or ("ç¬¬å››ä¸ª" in text)):
        return text
    if not re.search(_ORD_PATTERN, text):
        return text

    parts = re.split(_ORD_PATTERN, text)
    if len(parts) <= 1:
        return text

    lines = []
    current = ""
    for part in parts:
        if re.match(_ORD_PATTERN, part):
            if current.strip():
                lines.append(current.strip())
            current = part
        else:
            current += part

    if current.strip():
        lines.append(current.strip())

    return "\n".join(lines)


def preprocess_before_llm(raw_text: str) -> str:
    t = (raw_text or "").strip()
    if not t:
        return ""
    t = re.sub(r"[ \t]+", " ", t)
    t = split_ordered_items(t)
    t = t.replace("ã€‚-", "ã€‚\n-")
    return t.strip()


# =========================
# è¾“å‡ºå®‰å…¨é—¸é—¨ï¼šå­—ç¬¦ç›¸ä¼¼åº¦ + ngram è¦†ç›–ç‡ï¼ˆå®æ—¶å‹å¥½ï¼‰
# =========================
def normalize_for_guard(t: str) -> str:
    # å»æ‰ markdown ç¬¦å·ã€ç©ºç™½ã€å¸¸è§å™ªå£°
    t = (t or "")
    t = t.replace("\r", "\n")
    t = re.sub(r"[#*`>\-]", "", t)     # è½»åº¦å» markdown
    t = re.sub(r"\s+", "", t)
    return t

def build_ngrams(text: str, n: int = 3) -> set:
    text = normalize_for_guard(text)
    if len(text) < n:
        return set()
    return {text[i:i+n] for i in range(len(text) - n + 1)}

def ngram_coverage(source: str, target: str, n: int = 3) -> float:
    src = build_ngrams(source, n)
    if not src:
        return 0.0
    tgt = normalize_for_guard(target)
    if len(tgt) < n:
        return 0.0

    hit = 0
    total = 0
    for i in range(len(tgt) - n + 1):
        total += 1
        if tgt[i:i+n] in src:
            hit += 1
    return hit / total if total else 0.0

def is_llm_output_safe(raw_text: str, processed_text: str) -> bool:
    raw_n = normalize_for_guard(raw_text)
    out_n = normalize_for_guard(processed_text)
    if not raw_n or not out_n:
        return False

    sim = SequenceMatcher(None, raw_n, out_n).ratio()
    cov = ngram_coverage(raw_n, out_n, n=3)

    # ä½ å¯ä»¥æŠŠè¿™ä¸¤è¡Œ print æ‰“å¼€ï¼Œè°ƒå‚ç”¨
    # print(f"[guard] sim={sim:.3f} cov={cov:.3f}")

    if sim >= SAFE_SIM_HIGH:
        return True
    if sim >= SAFE_SIM_LOW and cov >= SAFE_NGRAM_COV:
        return True
    return False


# =========================
# LLMï¼šé€šç”¨è°ƒç”¨ï¼ˆåŠ  stopï¼Œå‡å°‘ # /think æ±¡æŸ“ï¼‰
# =========================
def call_ollama(prompt: str, timeout: int = 40) -> str:
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.0,
            "top_p": 0.75,
            "repeat_penalty": 1.15,
            # å…³é”®ï¼šä¸€æ—¦å¼€å§‹åè¿™äº›ï¼Œå°±æˆªæ–­
            "stop": ["\n#", "\n/think", "/think", "<think>", "</think>"],
        },
    }
    resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("response") or "").strip()


# =========================
# ä¼ ç»Ÿåå¤„ç†ï¼ˆclean/markdownï¼‰
# =========================
def build_prompt_edit(raw_text: str, mode: str) -> str:
    base_rules = (
        "ä½ æ˜¯ä¸€ä¸ªã€æ–‡æœ¬åå¤„ç†å™¨ã€‘ï¼Œåªåšç¼–è¾‘ï¼Œä¸åšè§£é‡Šã€‚\n"
        "åªå…è®¸ä¿®æ”¹åŸæ–‡è¡¨è¾¾ï¼Œä¸å…è®¸è¡¥å……ã€æ¨æµ‹ã€è§£é‡Šã€‚\n\n"
        "ç¼–è¾‘è§„åˆ™ï¼š\n"
        "1. åˆ é™¤å£è¯­å¡«å……è¯ã€é‡å¤è¯ï¼ˆå¦‚ï¼šå‘ƒã€å•Šã€ç„¶åã€å…¶å®ã€å°±æ˜¯ï¼‰ã€‚\n"
        "2. ä¿®æ­£æ˜æ˜¾é”™åˆ«å­—å’Œç—…å¥ï¼Œä½¿è¡¨è¾¾æ›´é€šé¡ºã€‚\n"
        "3. ä¸æ–°å¢ä»»ä½•ä¿¡æ¯ï¼Œä¸æ¨æµ‹ã€ä¸è¡¥å……ã€ä¸è¯„è®ºã€‚\n"
        "4. åªè¾“å‡ºæœ€ç»ˆç»“æœï¼Œä¸è¦è¾“å‡ºç¼–è¾‘è¯´æ˜ã€‚\n"
        "5. ç¦æ­¢è¾“å‡ºï¼š#ã€/thinkã€<think>ã€è§£é‡Šæ€§æ®µè½ã€‚\n"
    )

    if mode == "clean":
        fmt = (
            "è¾“å‡ºè¦æ±‚ï¼š\n"
            "- åªè¾“å‡ºä¸€æ®µè¿ç»­ä¸­æ–‡æ–‡æœ¬ã€‚\n"
            "- ä¸è¦æ ‡é¢˜ï¼Œä¸è¦åˆ—è¡¨ï¼Œä¸è¦ç©ºè¡Œã€‚\n"
        )
    else:
        fmt = (
            "è¾“å‡ºè¦æ±‚ï¼š\n"
            "- ä»…åœ¨åŸæ–‡æœ¬æœ¬èº«æ˜æ˜¾æ˜¯åˆ—ç‚¹æ—¶ï¼Œæ‰ä½¿ç”¨åˆ—è¡¨ç¬¦å·ï¼ˆ- æˆ– 1.ï¼‰ã€‚\n"
            "- ä¸è¦è¾“å‡ºè¯´æ˜æ€§è¯è¯­ï¼Œä¸è¦è§£é‡Šã€‚\n"
            "- ä¸è¦ä½¿ç”¨ # ä½œä¸ºæ ‡é¢˜ã€‚\n"
        )

    return base_rules + fmt + "\nåŸå§‹æ–‡æœ¬å¦‚ä¸‹ï¼š\n" + raw_text.strip()


def call_ollama_postprocess(raw_text: str, mode: str) -> str:
    raw_text = (raw_text or "").strip()
    if not raw_text:
        return ""
    prompt = build_prompt_edit(raw_text, mode)
    try:
        text = call_ollama(prompt, timeout=40)
        return text if text else raw_text
    except Exception as e:
        print("âš ï¸ Ollama call failed:", repr(e))
        return raw_text


# =========================
# Step2ï¼šç»“æ„åŒ–ç†è§£ï¼ˆLLM è¾“å‡º JSONï¼‰
# =========================
def build_prompt_struct(raw_text: str) -> str:
    return (
        "ä½ æ˜¯ä¸€ä¸ªã€ç»“æ„é‡æ’å™¨ã€‘ï¼Œä¸æ˜¯è§£é‡Šå™¨ã€ä¸æ˜¯æ€»ç»“å™¨ã€‚\n"
        "åªåšï¼šæ‹†åˆ†ã€æ¢è¡Œã€åˆ†ç»„ã€‚ç¦æ­¢ï¼šæ¨æµ‹ã€è§£é‡Šã€è¡¥å…¨ã€‚\n\n"
        "ç¡¬æ€§çº¦æŸï¼š\n"
        "1) åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œé™¤æ­¤ä¹‹å¤–ä¸è¦è¾“å‡ºä»»ä½•å­—ç¬¦ã€‚\n"
        "2) ä¸æ–°å¢äº‹å®ï¼Œä¸æ¨æµ‹ï¼Œä¸è¡¥å……æœªæåŠä¿¡æ¯ã€‚\n"
        "3) æ¯æ¡è¦ç‚¹å°½é‡çŸ­ï¼Œä¸€å¥è¯ä¸€ä¸ªè¦ç‚¹ã€‚\n"
        "4) æœ€å¤šä¸¤å±‚ï¼šbullets + subã€‚\n"
        "5) ç¦æ­¢è¾“å‡ºï¼š#ã€/thinkã€<think>ã€è§£é‡Šæ€§å¥å­ï¼ˆå¦‚â€œè¯¢é—®/æ˜¯å¦/å¯èƒ½/ç”¨äº/è¡¨ç¤ºâ€ï¼‰ã€‚\n\n"
        "JSON ç»“æ„å¿…é¡»ä¸¥æ ¼ä¸ºï¼š\n"
        "{\"title\":\"\",\"bullets\":[{\"text\":\"\",\"sub\":[{\"text\":\"\"}]}]}\n\n"
        "åŸå§‹æ–‡æœ¬ï¼š\n"
        + raw_text.strip()
    )

def strip_formatting(text: str) -> str:
    """
    å»æ‰æ‰€æœ‰æ’ç‰ˆä¿¡æ¯ï¼Œåªä¿ç•™â€œçº¯å†…å®¹â€
    """
    if not text:
        return ""

    t = text
    # å» markdown ç¬¦å·
    t = re.sub(r"^\s*-\s*", "", t, flags=re.MULTILINE)
    t = re.sub(r"^\s*\*\*|\*\*\s*$", "", t, flags=re.MULTILINE)
    t = re.sub(r"[#*`>]", "", t)

    # æŠŠæ¢è¡Œå½“æˆç©ºæ ¼
    t = re.sub(r"\n+", " ", t)

    # å‹ç¼©ç©ºç™½
    t = re.sub(r"\s+", " ", t)

    return t.strip()

def extract_first_json(text: str) -> str:
    """æ‹¬å·åŒ¹é…æŠ½å–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡"""
    if not text:
        return ""
    start = text.find("{")
    if start < 0:
        return ""
    depth = 0
    for i in range(start, len(text)):
        ch = text[i]
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                return text[start:i+1].strip()
    return ""


def parse_outline(json_text: str):
    """è§£æå¹¶æ ¡éªŒ outline ç»“æ„"""
    obj = json.loads(json_text)
    if not isinstance(obj, dict):
        return None

    title = obj.get("title", "")
    bullets = obj.get("bullets", [])

    if not isinstance(title, str):
        title = ""
    if not isinstance(bullets, list):
        return None

    cleaned = []
    for b in bullets:
        if not isinstance(b, dict):
            continue
        text = b.get("text", "")
        if not isinstance(text, str):
            continue
        text = text.strip()
        if not text:
            continue

        sub_list = b.get("sub", [])
        sub_clean = []
        if isinstance(sub_list, list):
            for s in sub_list:
                st = s.get("text", "") if isinstance(s, dict) else s
                if isinstance(st, str):
                    st = st.strip()
                    if st:
                        sub_clean.append({"text": st})

        cleaned.append({"text": text, "sub": sub_clean})

    if not cleaned:
        return None

    cleaned = cleaned[:8]
    for b in cleaned:
        b["sub"] = b["sub"][:8]

    return {"title": title.strip(), "bullets": cleaned}


def outline_to_markdown(outline: dict) -> str:
    """å·¥ç¨‹æ¸²æŸ“ Markdownï¼ˆç¨³å®šï¼‰"""
    lines = []
    title = (outline.get("title") or "").strip()
    if title:
        lines.append(f"**{title}**")

    for b in outline.get("bullets", []):
        lines.append(f"- {b['text']}")
        for s in b.get("sub", []):
            lines.append(f"  - {s['text']}")

    return "\n".join(lines).strip()


def smart_struct_then_render(raw_text: str) -> str:
    """ä¸¤é˜¶æ®µï¼šç»“æ„åŒ–(JSON) -> å·¥ç¨‹æ¸²æŸ“ Markdownï¼›å¤±è´¥è¿”å›ç©ºä¸²"""
    raw_text = (raw_text or "").strip()
    if not raw_text:
        return ""

    pre = preprocess_before_llm(raw_text)

    try:
        prompt = build_prompt_struct(pre)
        resp = call_ollama(prompt, timeout=50)
        js = extract_first_json(resp)
        if not js:
            return ""

        outline = parse_outline(js)
        if not outline:
            return ""

        md = outline_to_markdown(outline)
        return md.strip() if md.strip() else ""
    except Exception as e:
        print("âš ï¸ smart_struct_then_render failed:", repr(e))
        return ""


# =========================
# 1. åˆå§‹åŒ– ASR æ¨¡å‹
# =========================
model = AutoModel(
    model="paraformer-zh-streaming",
    device="mps"
)

# =========================
# 2. æµå¼å‚æ•°
# =========================
sample_rate = 16000
chunk_size = [0, 10, 5]
encoder_chunk_look_back = 4
decoder_chunk_look_back = 1
chunk_stride = chunk_size[1] * 960

cache = {}
audio_buffer = np.zeros((0,), dtype=np.float32)
last_text = ""

text_queue = Queue()

# =========================
# 3. è¿è¡Œæ—¶çŠ¶æ€ï¼ˆæ ¸å¿ƒï¼špreview + commitï¼‰
# =========================
state_lock = threading.Lock()

preview_raw_text = ""
preview_len = 0
last_voice_time = time.time()
last_commit_time = 0.0
committing = False


# =========================
# 4. éŸ³é¢‘å›è°ƒï¼ˆåªè´Ÿè´£ ASR + èƒ½é‡æ£€æµ‹ + æ¨é€å¢é‡ï¼‰
# =========================
def record_callback(indata, frames, time_info, status):
    global audio_buffer, last_text, last_voice_time

    audio_mono = indata[:, 0].astype(np.float32)
    rms = float(np.sqrt(np.mean(audio_mono * audio_mono)) + 1e-12)
    if rms > ENERGY_THRESHOLD:
        last_voice_time = time.time()

    audio_buffer = np.concatenate([audio_buffer, audio_mono])

    while len(audio_buffer) >= chunk_stride:
        chunk = audio_buffer[:chunk_stride]
        audio_buffer = audio_buffer[chunk_stride:]

        res = model.generate(
            input=chunk,
            cache=cache,
            is_final=False,
            chunk_size=chunk_size,
            encoder_chunk_look_back=encoder_chunk_look_back,
            decoder_chunk_look_back=decoder_chunk_look_back,
        )

        if not res or not res[0].get("text"):
            return

        text = res[0]["text"]
        new_part = diff_new_part(last_text, text)

        if new_part.strip():
            text_queue.put(new_part)

        last_text = text


# =========================
# 5. Commitï¼šé™éŸ³åè§¦å‘ LLM â†’ å®‰å…¨é—¸é—¨ â†’ æ›¿æ¢ preview
# =========================
def try_commit_if_needed():
    global preview_raw_text, preview_len, last_commit_time, committing

    now = time.time()

    with state_lock:
        if committing:
            return
        if preview_len <= 0:
            return
        if now - last_voice_time < SILENCE_TIMEOUT:
            return
        if now - last_commit_time < MIN_COMMIT_GAP:
            return

        committing = True
        raw_to_process = preview_raw_text
        chars_to_delete = preview_len

    print("\nğŸ§  commit trigger -> post-process...")

    raw_clean = preprocess_before_llm(raw_to_process)

    processed = ""
    if LLM_MODE == "smart_markdown":
        processed = smart_struct_then_render(raw_clean)

        # å®‰å…¨é—¸é—¨ï¼šæŒ¡æ‰æ¨æµ‹æ€§è¾“å‡º
        if processed:
            raw_for_guard = strip_formatting(raw_to_process)
            out_for_guard = strip_formatting(processed)

            if not is_llm_output_safe(raw_for_guard, out_for_guard):
                print("ğŸ§¯ guard rejected output -> fallback clean")
                processed = ""

        if not processed:
            processed = call_ollama_postprocess(raw_clean, mode="clean").strip()

    else:
        processed = call_ollama_postprocess(raw_clean, LLM_MODE).strip()

        if processed and not is_llm_output_safe(raw_to_process, processed):
            print("ğŸ§¯ guard rejected output -> keep raw")
            processed = raw_to_process

    if not processed:
        processed = raw_to_process

    delete_chars(chars_to_delete)
    paste_text(processed)

    with state_lock:
        preview_raw_text = ""
        preview_len = 0
        last_commit_time = time.time()
        committing = False

    print("âœ… commit done\n")


# =========================
# 6. å¯åŠ¨éº¦å…‹é£ & ä¸»çº¿ç¨‹è¾“å‡º
# =========================
print("ğŸ™ è¯·æŠŠå…‰æ ‡æ”¾åœ¨ä»»æ„è¾“å…¥æ¡†ï¼ˆå¾®ä¿¡ / è®°äº‹æœ¬ / æµè§ˆå™¨éƒ½è¡Œï¼‰")
print("ğŸ‘‰ preview å®æ—¶å‡ºå­—ï¼Œåœé¡¿å commit ä¼šç”¨ç»“æ„åŒ–+ç¾åŒ–æ›¿æ¢ï¼ˆå¸¦å®‰å…¨é—¸é—¨ï¼‰")
print(f"ğŸ‘‰ æ¨¡å¼ï¼š{LLM_MODE} | é™éŸ³é˜ˆå€¼ï¼š{SILENCE_TIMEOUT}s | æ¨¡å‹ï¼š{OLLAMA_MODEL}")


with sd.InputStream(
    samplerate=sample_rate,
    channels=1,
    dtype="float32",
    blocksize=1024,
    callback=record_callback,
):
    try:
        while True:
            while not text_queue.empty():
                new_text = text_queue.get()

                paste_text(new_text)

                with state_lock:
                    preview_raw_text += new_text
                    preview_len += len(new_text)

            try_commit_if_needed()
            sd.sleep(20)

    except KeyboardInterrupt:
        print("\nğŸ›‘ stopped")