import sounddevice as sd
import numpy as np
import pyautogui
import pyperclip
import requests
from funasr import AutoModel
from queue import Queue, Empty
import time
import threading
import re
import json
from difflib import SequenceMatcher

# =========================
# å‚æ•°åŒºï¼ˆä½ åé¢è°ƒå‚å°±è°ƒè¿™é‡Œï¼‰
# =========================
SILENCE_TIMEOUT = 0.6          # é™éŸ³è¶…è¿‡å¤šå°‘ç§’ -> commit
ENERGY_THRESHOLD = 0.008       # é™éŸ³èƒ½é‡é˜ˆå€¼ï¼ˆä¸åŒéº¦å…‹é£è¦è°ƒï¼Œåå°æ›´æ•æ„Ÿï¼‰
MIN_COMMIT_GAP = 0.8           # ä¸¤æ¬¡ commit æœ€å°é—´éš”ï¼ˆé˜²æŠ–ï¼‰

OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen3:1.7b"

LLM_MODE = "smart_markdown"    # "clean" / "markdown" / "smart_markdown"

# è¾“å‡ºå®‰å…¨é—¸é—¨é˜ˆå€¼ï¼ˆæ›´é€‚é…â€œç»“æ„é‡æ’â€ï¼‰
SAFE_SIM_HIGH = 0.70
SAFE_SIM_LOW  = 0.52
SAFE_NGRAM_COV = 0.48
SAFE_LEN_RATIO_MIN = 0.55
SAFE_LEN_RATIO_MAX = 1.60

# ç²˜è´´èŠ‚å¥
pyautogui.PAUSE = 0.005


# =========================
# å·¥å…·ï¼šdiff æ–°å¢æ–‡æœ¬
# =========================
def diff_new_part(prev: str, curr: str) -> str:
    i = 0
    while i < len(prev) and i < len(curr) and prev[i] == curr[i]:
        i += 1
    return curr[i:]


# =========================
# å·¥å…·ï¼špasteï¼ˆæ ¸å¿ƒï¼‰
# =========================
def paste_text(text: str):
    if not text:
        return
    pyperclip.copy(text)
    pyautogui.hotkey("command", "v")


# =========================
# å·¥å…·ï¼šå›åˆ  N ä¸ªå­—ç¬¦ï¼ˆç”¨äºæ›¿æ¢ previewï¼‰
# =========================
def delete_chars(n: int):
    if n <= 0:
        return
    pyautogui.press("backspace", presses=n, interval=0)


# =========================
# Step0ï¼šå·¥ç¨‹çº§å»å£è¯­/è¯­æ°”è¯ï¼ˆå·²ç§»é™¤ï¼Œæ”¹ä¸ºLLMå¤„ç†ï¼‰
# =========================
# è¯­æ°”è¯å¤„ç†å·²æ”¹ä¸ºé€šè¿‡LLMæç¤ºè¯å®Œæˆï¼Œä¸å†ä½¿ç”¨å·¥ç¨‹æ–¹å¼


# =========================
# Step1ï¼šå·¥ç¨‹é¢„æ¸…æ´—ï¼ˆç»“æ„åŒ–å‰æ°å¼€ç²˜è¿ï¼‰
# =========================
_ORD_PATTERN = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ä¸ª\s*[:ï¼š])'

def split_ordered_items(text: str) -> str:
    """æŠŠ 'ç¬¬ä¸€ä¸ªï¼šxxç¬¬äºŒä¸ªï¼šyy' æ‹†æˆå¤šè¡Œï¼ˆä»…æ‹†ç»“æ„ï¼Œä¸æ–°å¢å†…å®¹ï¼‰"""
    if not text:
        return text
    if text.count("ç¬¬") < 2:
        return text
    if not (("ç¬¬äºŒä¸ª" in text) or ("ç¬¬ä¸‰ä¸ª" in text) or ("ç¬¬å››ä¸ª" in text)):
        return text
    if not re.search(_ORD_PATTERN, text):
        return text

    parts = re.split(_ORD_PATTERN, text)
    if len(parts) <= 1:
        return text

    lines = []
    current = ""
    for part in parts:
        if re.match(_ORD_PATTERN, part):
            if current.strip():
                lines.append(current.strip())
            current = part
        else:
            current += part

    if current.strip():
        lines.append(current.strip())

    return "\n".join(lines)

def add_soft_breaks(text: str) -> str:
    """åªåš"æ›´åˆ©äºç»“æ„åŒ–"çš„è½»åº¦æ¢è¡Œï¼Œä¸æ–°å¢ä¿¡æ¯"""
    if not text:
        return ""
    t = text
    
    # ä¼˜å…ˆå¤„ç†"ç¬¬Xç‚¹"æ¨¡å¼ï¼ˆåŒ…æ‹¬"ç¬¬ä¸€ç‚¹"ã€"ç¬¬äºŒç‚¹"ã€"ç¬¬ä¸‰ç‚¹"ç­‰ï¼‰
    # åŒ¹é…"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç‚¹"æˆ–"ç¬¬[0-9]+ç‚¹"
    t = re.sub(r"([ï¼Œã€‚ï¼ï¼Ÿ,\.\!\?\s]*)(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+ç‚¹)", r"\1\n\2", t)
    
    # å¸¸è§ç»“æ„è¿æ¥è¯/åºå·è¯å‰æ¢è¡Œï¼Œå¸®åŠ© LLM æ„ŸçŸ¥"åˆ†ç‚¹"
    keywords = [
        "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "å¦å¤–", "æœ€å",
        "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "ç¬¬å››", "ç¬¬äº”",
    ]
    for w in keywords:
        # æƒ…å†µ1ï¼šå¥å·/åˆ†å·ä¹‹å
        t = re.sub(rf"(ã€‚|ï¼›|;)\s*({w})", r"\1\n\2", t)
        # æƒ…å†µ2ï¼šè¡Œé¦–æˆ–ç©ºæ ¼ä¹‹åï¼ˆä½†é¿å…é‡å¤æ¢è¡Œï¼‰
        t = re.sub(rf"(^|\s+)({w})", r"\1\n\2", t)
    
    # æ¸…ç†å¤šä½™çš„è¿ç»­æ¢è¡Œ
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t

def preprocess_before_llm(raw_text: str) -> str:
    t = (raw_text or "").strip()
    if not t:
        return ""

    # åªåšåŸºæœ¬çš„ç©ºç™½è§„èŒƒåŒ–ï¼Œä¿æŒåŸå§‹æ ¼å¼ï¼ˆæ¢è¡Œã€åˆ—è¡¨ç­‰ï¼‰
    t = re.sub(r"[ \t]+", " ", t)
    # è¯­æ°”è¯å¤„ç†å·²æ”¹ä¸ºé€šè¿‡LLMæç¤ºè¯å®Œæˆ
    t = split_ordered_items(t)
    t = add_soft_breaks(t)

    # ä½ ä¹‹å‰çš„å¤„ç†ä¿ç•™
    t = t.replace("ã€‚-", "ã€‚\n-")

    # ç»Ÿä¸€æ¢è¡Œ
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()


# =========================
# è¾“å‡ºå®‰å…¨é—¸é—¨ï¼šæ›´é€‚é…ç»“æ„é‡æ’
# =========================
def normalize_for_guard(t: str) -> str:
    t = (t or "").replace("\r", "\n")
    # è½»åº¦å» markdown
    t = re.sub(r"[#*`>\-]", "", t)
    # å»ç©ºç™½
    t = re.sub(r"\s+", "", t)
    return t

def build_ngrams(text: str, n: int = 3) -> set:
    text = normalize_for_guard(text)
    if len(text) < n:
        return set()
    return {text[i:i+n] for i in range(len(text) - n + 1)}

def ngram_coverage(source: str, target: str, n: int = 3) -> float:
    src = build_ngrams(source, n)
    if not src:
        return 0.0
    tgt = normalize_for_guard(target)
    if len(tgt) < n:
        return 0.0

    hit = 0
    total = 0
    for i in range(len(tgt) - n + 1):
        total += 1
        if tgt[i:i+n] in src:
            hit += 1
    return hit / total if total else 0.0

def is_llm_output_safe(raw_text, out_text, mode="format"):
    """
    LLM è¾“å‡ºå®‰å…¨æ£€æŸ¥

    mode:
      - format  : åŸæœ‰çš„â€œæ ¼å¼åŒ– / æ¸…æ´—â€æ¨¡å¼ï¼ˆåä¸¥æ ¼ï¼‰
      - reorder : ç»“æ„é‡æ’æ¨¡å¼ï¼ˆå…è®¸é‡æ’ã€åˆ†ç»„ã€ç»“æ„è¯ï¼‰
    """

    # åŸºç¡€å…œåº•
    if not out_text or not out_text.strip():
        return False

    raw_text = raw_text or ""

    # =========================
    # ç»“æ„é‡æ’æ¨¡å¼ï¼ˆæ–°ï¼‰
    # =========================
    if mode == "reorder":
        # 1ï¸âƒ£ è¾“å‡ºä¸èƒ½æç«¯è†¨èƒ€ï¼ˆé˜²èƒ¡ç¼–ï¼‰
        if len(out_text) > len(raw_text) * 3:
            return False

        # 2ï¸âƒ£ ä¸èƒ½å®Œå…¨è„±ç¦»åŸæ–‡ï¼ˆè¯æ±‡å®Œå…¨ä¸é‡åˆï¼‰
        raw_tokens = set(raw_text.replace("\n", " ").split())
        out_tokens = set(out_text.replace("\n", " ").split())

        # å…è®¸çš„ç»“æ„è¯ï¼ˆç™½åå•ï¼Œå¯æ…¢æ…¢åŠ ï¼‰
        structural_tokens = {
            "-", "*", "ï¼š", ":",
            "è¦ç‚¹", "å­è¦ç‚¹", "åŠŸèƒ½", "ç‰¹ç‚¹", "æè¿°", "è¯´æ˜"
        }

        out_tokens = {t for t in out_tokens if t not in structural_tokens}

        if not raw_tokens:
            return True  # åŸæ–‡å¤ªçŸ­ï¼Œç›´æ¥æ”¾è¡Œ

        overlap_ratio = len(raw_tokens & out_tokens) / max(1, len(out_tokens))

        # ç»éªŒé˜ˆå€¼ï¼š30% å·²ç»å¾ˆå®½æ¾
        return overlap_ratio >= 0.3

    # =========================
    # åŸæœ‰æ ¼å¼åŒ–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    # =========================
    # ğŸ‘‰ ä¿ç•™ä½ åŸæ¥çš„é€»è¾‘å³å¯
    # ä¸‹é¢æ˜¯ä¸€ä¸ªâ€œä¿å®ˆç¤ºä¾‹â€ï¼Œä½ å¯ä»¥æ›¿æ¢æˆä½ åŸæ¥çš„å®ç°
    else:
        raw_simple = strip_formatting(raw_text)
        out_simple = strip_formatting(out_text)

        if not out_simple:
            return False

        # ç®€å•ç›¸ä¼¼åº¦å…œåº•ï¼ˆç¤ºæ„ï¼‰
        if len(out_simple) < len(raw_simple) * 0.3:
            return False

        return True

# è¿™æ˜¯ä¸€ä¸ªæˆ‘æœ¬åœ°éƒ¨ç½²çš„iè¯­éŸ³ç³»ç»Ÿ- å¯¹ä¸€ä¸ªè¯­éŸ³è¾“å…¥æ³•ï¼Œå¯ä»¥å»é™¤å£éŸ³ï¼ŒåŒæ—¶è¿›è¡Œå…¶ä»–æ“ä½œæ ¼å¼åŒ–çš„å±•ç¤ºç„¶åä»¥åŠåŒ…æ‹¬åƒä¸€äº›è¦ç‚¹æ¯”å¦‚è¯´ç¬¬ä¸€ç‚¹æ˜¯é‚£ä¸ª


# =========================
# LLMï¼šé€šç”¨è°ƒç”¨ï¼ˆåŠ  stopï¼Œå‡å°‘ # /think æ±¡æŸ“ï¼‰
# =========================
def call_ollama(prompt: str, timeout: int = 40) -> str:
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.0,
            "top_p": 0.75,
            "repeat_penalty": 1.15
        },
    }
    resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    return (data.get("response") or "").strip()


# =========================
# ä¼ ç»Ÿåå¤„ç†ï¼ˆclean/markdownï¼‰
# =========================
def build_prompt_edit(raw_text: str, mode: str) -> str:
    base_rules = (
        "ä½ æ˜¯ä¸€ä¸ªã€æ–‡æœ¬åå¤„ç†å™¨ã€‘ï¼Œåªåšç¼–è¾‘ï¼Œä¸åšè§£é‡Šã€‚\n"
        "åªå…è®¸ä¿®æ”¹åŸæ–‡è¡¨è¾¾ï¼Œä¸å…è®¸è¡¥å……ã€æ¨æµ‹ã€è§£é‡Šã€‚\n\n"
        "ç¼–è¾‘è§„åˆ™ï¼š\n"
        "1. åˆ é™¤å£è¯­å¡«å……è¯ã€è¯­æ°”è¯ã€é‡å¤è¯ï¼ˆå¦‚ï¼šå—¯ã€å‘ƒã€å•Šã€é‚£ä¸ªã€è¿™ä¸ªã€ç„¶åã€å…¶å®ã€å°±æ˜¯ã€ä½ çŸ¥é“ã€å°±æ˜¯è¯´ï¼‰ã€‚\n"
        "2. ä¿®æ­£æ˜æ˜¾é”™åˆ«å­—å’Œç—…å¥ï¼Œä½¿è¡¨è¾¾æ›´é€šé¡ºã€‚\n"
        "3. ä¸æ–°å¢ä»»ä½•ä¿¡æ¯ï¼Œä¸æ¨æµ‹ã€ä¸è¡¥å……ã€ä¸è¯„è®ºã€‚\n"
        "4. å¿…é¡»ä¿æŒåŸæ–‡çš„æ ¼å¼ç»“æ„ï¼šä¿ç•™æ‰€æœ‰æ¢è¡Œã€åˆ—è¡¨ç¬¦å·ï¼ˆ-ã€1.ç­‰ï¼‰ã€æ®µè½åˆ†éš”ã€‚\n"
        "5. å¦‚æœåŸæ–‡æœ‰åˆ—è¡¨ç»“æ„ï¼ˆæœ‰åºæˆ–æ— åºï¼‰ï¼Œå¿…é¡»ä¿æŒåˆ—è¡¨æ ¼å¼ï¼Œä¸è¦åˆå¹¶æˆä¸€æ®µã€‚\n"
        "6. åªè¾“å‡ºæœ€ç»ˆç»“æœï¼Œä¸è¦è¾“å‡ºç¼–è¾‘è¯´æ˜ã€‚\n"
        "7. ç¦æ­¢è¾“å‡ºï¼š#ã€/thinkã€<think>ã€è§£é‡Šæ€§æ®µè½ã€‚\n"
    )

    if mode == "clean":
        # å…³é”®ä¿®å¤ï¼šå¿…é¡»ä¿æŒæ ¼å¼åŒ–ï¼Œä¸è¦åˆå¹¶æˆä¸€æ®µï¼Œå¼ºåˆ¶è¯†åˆ«åˆ—è¡¨ç»“æ„
        fmt = (
            "è¾“å‡ºè¦æ±‚ï¼š\n"
            "- è¾“å‡ºä¸ºé€šé¡ºä¸­æ–‡ã€‚\n"
            "- å¦‚æœåŸæ–‡åŒ…å«'ç¬¬ä¸€ç‚¹'ã€'ç¬¬äºŒç‚¹'ã€'ç¬¬ä¸‰ç‚¹'ã€'é¦–å…ˆ'ã€'å…¶æ¬¡'ã€'æœ€å'ç­‰åˆ—è¡¨æ ‡è¯†è¯ï¼Œå¿…é¡»æ ¼å¼åŒ–ä¸ºåˆ—è¡¨ï¼Œæ¯é¡¹å•ç‹¬ä¸€è¡Œã€‚\n"
            "- åˆ—è¡¨æ ¼å¼ï¼šä½¿ç”¨ '- ' æˆ– '1. ' å¼€å¤´ï¼Œæ¯ä¸€ç‚¹ç‹¬ç«‹ä¸€è¡Œã€‚\n"
            "- å¿…é¡»ä¿ç•™åŸæ–‡çš„æ‰€æœ‰æ¢è¡Œå’Œæ®µè½åˆ†éš”ã€‚\n"
            "- å¦‚æœåŸæ–‡æœ‰åˆ—è¡¨ç»“æ„ï¼ˆæœ‰åºæˆ–æ— åºï¼‰ï¼Œå¿…é¡»ä¿æŒåˆ—è¡¨æ ¼å¼ï¼Œä¸è¦åˆå¹¶æˆä¸€æ®µã€‚\n"
            "- ç»å¯¹ä¸è¦å°†æ‰€æœ‰å†…å®¹åˆå¹¶æˆä¸€æ®µè¿ç»­æ–‡æœ¬ã€‚\n"
            "- ä¸è¦å†™æ ‡é¢˜ï¼Œä¸è¦å†™è§£é‡Šã€‚\n"
        )
    else:
        fmt = (
            "è¾“å‡ºè¦æ±‚ï¼š\n"
            "- å¦‚æœåŸæ–‡åŒ…å«'ç¬¬ä¸€ç‚¹'ã€'ç¬¬äºŒç‚¹'ã€'ç¬¬ä¸‰ç‚¹'ã€'é¦–å…ˆ'ã€'å…¶æ¬¡'ã€'æœ€å'ç­‰åˆ—è¡¨æ ‡è¯†è¯ï¼Œå¿…é¡»æ ¼å¼åŒ–ä¸ºåˆ—è¡¨ï¼Œæ¯é¡¹å•ç‹¬ä¸€è¡Œã€‚\n"
            "- åˆ—è¡¨æ ¼å¼ï¼šä½¿ç”¨ '- ' æˆ– '1. ' å¼€å¤´ï¼Œæ¯ä¸€ç‚¹ç‹¬ç«‹ä¸€è¡Œã€‚\n"
            "- å³ä½¿åŸæ–‡åªæœ‰'ç¬¬Xç‚¹'ï¼ˆå¦‚'ç¬¬ä¸‰ç‚¹æ˜¯å¯ä»¥åšè¿™ä¸ª'ï¼‰ï¼Œä¹Ÿè¦æ ¼å¼åŒ–ä¸ºåˆ—è¡¨é¡¹ï¼ˆå¦‚ '- ç¬¬ä¸‰ç‚¹æ˜¯å¯ä»¥åšè¿™ä¸ª'ï¼‰ã€‚\n"
            "- å¦‚æœåŸæ–‡æœ¬æœ¬èº«æ˜¯åˆ—ç‚¹ç»“æ„ï¼Œå¿…é¡»ä½¿ç”¨åˆ—è¡¨ç¬¦å·ï¼ˆ- æˆ– 1.ï¼‰ï¼Œæ¯é¡¹å•ç‹¬ä¸€è¡Œã€‚\n"
            "- å¿…é¡»ä¿ç•™æˆ–æ¢å¤æ‰€æœ‰åˆç†çš„æ¢è¡Œå’Œæ®µè½åˆ†éš”ã€‚\n"
            "- ç»å¯¹ä¸è¦å°†æ‰€æœ‰å†…å®¹åˆå¹¶æˆä¸€æ®µè¿ç»­æ–‡æœ¬ã€‚\n"
            "- ä¸è¦è¾“å‡ºè¯´æ˜æ€§è¯è¯­ï¼Œä¸è¦è§£é‡Šã€‚\n"
            "- ä¸è¦ä½¿ç”¨ # ä½œä¸ºæ ‡é¢˜ã€‚\n"
        )

    return base_rules + fmt + "\nåŸå§‹æ–‡æœ¬å¦‚ä¸‹ï¼š\n" + (raw_text or "").strip()

def call_ollama_postprocess(raw_text: str, mode: str) -> str:
    raw_text = (raw_text or "").strip()
    if not raw_text:
        return ""
    prompt = build_prompt_edit(raw_text, mode)
    try:
        text = call_ollama(prompt, timeout=40)
        return text if text else raw_text
    except Exception as e:
        print("âš ï¸ Ollama call failed:", repr(e))
        return raw_text


# =========================
# Step2ï¼šç»“æ„åŒ–ç†è§£ï¼ˆLLM è¾“å‡º JSONï¼‰
# =========================

def build_prompt_reorder(raw_text: str) -> str:
    return (
        "ä½ æ˜¯ä¸€ä¸ªã€æ–‡æœ¬ç»“æ„é‡æ’å™¨ã€‘ï¼Œä¸æ˜¯æ€»ç»“å™¨ã€ä¸æ˜¯è§£é‡Šå™¨ã€‚\n"
        "ç›®æ ‡ï¼šæŠŠå£è¯­åŒ–ã€é›¶æ•£çš„è¡¨è¾¾ï¼Œé‡æ’ä¸ºé€»è¾‘æ¸…æ™°çš„ç»“æ„åŒ–æ–‡æœ¬ã€‚\n\n"

        "åªå…è®¸åšçš„äº‹æƒ…ï¼š\n"
        "- åˆ é™¤å£è¯­å¡«å……è¯ï¼ˆå¦‚ï¼šå—¯ã€å‘ƒã€å•Šã€é‚£ä¸ªã€ç„¶åã€å…¶å®ã€å°±æ˜¯ï¼‰\n"
        "- åˆå¹¶é‡å¤æ„æ€\n"
        "- æ‹†åˆ†é•¿å¥\n"
        "- è°ƒæ•´é¡ºåºï¼Œè®©è¡¨è¾¾æ›´æ¸…æ™°\n\n"

        "ç¦æ­¢ï¼š\n"
        "- æ–°å¢äº‹å®\n"
        "- æ¨æµ‹åŸæ–‡æœªæåŠçš„å†…å®¹\n"
        "- æ€»ç»“ã€å‡åã€è¯„ä»·\n\n"

        "è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š\n"
        "- ä½¿ç”¨ Markdown\n"
        "- ä¸€çº§ç»“æ„ä½¿ç”¨æ— åºåˆ—è¡¨ `-`\n"
        "- å­ç»“æ„ä½¿ç”¨ç¼©è¿›ä¸¤æ ¼çš„ `-`\n"
        "- ä¸è¦ä½¿ç”¨æ ‡é¢˜ç¬¦å· `#`\n"
        "- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—\n\n"

        "ç¤ºä¾‹æ ¼å¼ï¼š\n"
        "- è¦ç‚¹ä¸€\n"
        "  - å­è¦ç‚¹\n"
        "- è¦ç‚¹äºŒ\n\n"

        "åŸå§‹æ–‡æœ¬ï¼š\n"
        + (raw_text or "").strip()
    )
def build_prompt_struct(raw_text: str) -> str:
    return (
        "ä½ æ˜¯ä¸€ä¸ªã€ç»“æ„é‡æ’å™¨ã€‘ï¼Œä¸æ˜¯è§£é‡Šå™¨ã€ä¸æ˜¯æ€»ç»“å™¨ã€‚\n"
        "åªåšï¼šåˆ é™¤å£è¯­å¡«å……è¯ã€æ‹†åˆ†ã€æ¢è¡Œã€åˆ†ç»„ã€‚ç¦æ­¢ï¼šæ¨æµ‹ã€è§£é‡Šã€è¡¥å…¨ã€‚\n\n"
        "ç¡¬æ€§çº¦æŸï¼š\n"
        "1) åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œé™¤æ­¤ä¹‹å¤–ä¸è¦è¾“å‡ºä»»ä½•å­—ç¬¦ã€‚\n"
        "2) ä¸æ–°å¢äº‹å®ï¼Œä¸æ¨æµ‹ï¼Œä¸è¡¥å……æœªæåŠä¿¡æ¯ã€‚\n"
        "3) åˆ é™¤å£è¯­å¡«å……è¯/è¯­æ°”è¯/å£å¤´ç¦…ï¼ˆå¦‚ï¼šå—¯ã€å‘ƒã€å•Šã€é‚£ä¸ªã€è¿™ä¸ªã€ç„¶åã€å…¶å®ã€å°±æ˜¯ã€ä½ çŸ¥é“ã€å°±æ˜¯è¯´ï¼‰ã€‚\n"
        "4) æ¯æ¡è¦ç‚¹å°½é‡çŸ­ï¼Œä¸€å¥è¯ä¸€ä¸ªè¦ç‚¹ã€‚\n"
        "5) å¿…é¡»ä¿æŒç»“æ„åŒ–è¾“å‡ºï¼šå¦‚æœåŸæ–‡æœ‰åˆ—è¡¨ç»“æ„ï¼Œå¿…é¡»åœ¨JSONä¸­æ­£ç¡®åˆ†ç»„ä¸ºbulletså’Œsubã€‚\n"
        "6) æœ€å¤šä¸¤å±‚ï¼šbullets + subã€‚\n"
        "7) ç¦æ­¢è¾“å‡ºï¼š#ã€/thinkã€<think>ã€è§£é‡Šæ€§å¥å­ã€‚\n\n"
        "JSON ç»“æ„å¿…é¡»ä¸¥æ ¼ä¸ºï¼š\n"
        "{\"title\":\"\",\"bullets\":[{\"text\":\"\",\"sub\":[{\"text\":\"\"}]}]}\n\n"
        "åŸå§‹æ–‡æœ¬ï¼š\n"
        + (raw_text or "").strip()
    )

def strip_formatting(text: str) -> str:
    """
    ä»…ç”¨äº guard æ¯”å¯¹ï¼šå»æ‰æ’ç‰ˆç¬¦å·ã€æŠŠæ¢è¡Œå½“ç©ºæ ¼
    """
    if not text:
        return ""
    t = text
    t = re.sub(r"^\s*-\s*", "", t, flags=re.MULTILINE)
    t = re.sub(r"[#*`>]", "", t)
    t = re.sub(r"\n+", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def clean_json_string(json_str: str) -> str:
    """æ¸…ç†å’Œä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é—®é¢˜"""
    if not json_str:
        return ""
    
    # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
    json_str = re.sub(r"^```(?:json)?\s*", "", json_str, flags=re.MULTILINE)
    json_str = re.sub(r"```\s*$", "", json_str, flags=re.MULTILINE)
    
    # ç§»é™¤å‰åçš„é JSON å­—ç¬¦ï¼ˆä¿ç•™å¯èƒ½çš„ç©ºç™½ï¼‰
    json_str = json_str.strip()
    
    # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯
    # 1. ä¿®å¤é”®åä¸­çš„å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™ï¼‰
    # åŒ¹é… 'key': æˆ– 'key' : è¿™ç§æ¨¡å¼
    json_str = re.sub(r"'([^']+)'\s*:", r'"\1":', json_str)
    
    # 2. ç§»é™¤å°¾éšé€—å·ï¼ˆåœ¨ } æˆ– ] å‰ï¼Œä½†è¦å°å¿ƒå­—ç¬¦ä¸²ä¸­çš„é€—å·ï¼‰
    # ä½¿ç”¨è´Ÿå‘å‰ç»ç¡®ä¿ä¸åœ¨å­—ç¬¦ä¸²å†…
    json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
    
    # 3. ç§»é™¤å¯èƒ½çš„æ³¨é‡Šï¼ˆè™½ç„¶ JSON æ ‡å‡†ä¸æ”¯æŒï¼‰
    json_str = re.sub(r'//.*?$', '', json_str, flags=re.MULTILINE)
    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
    
    return json_str.strip()

def extract_first_json(text: str) -> str:
    if not text:
        return ""
    start = text.find("{")
    if start < 0:
        return ""
    depth = 0
    for i in range(start, len(text)):
        ch = text[i]
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                extracted = text[start:i+1].strip()
                # æ¸…ç†æå–çš„ JSON
                return clean_json_string(extracted)
    return ""

def parse_outline(json_text: str):
    if not json_text:
        return None
    
    # å…ˆå°è¯•ç›´æ¥è§£æ
    try:
        obj = json.loads(json_text)
    except json.JSONDecodeError as e:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ¸…ç†åå†è§£æ
        cleaned = clean_json_string(json_text)
        try:
            obj = json.loads(cleaned)
        except json.JSONDecodeError:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"[debug] JSON parse failed at position {e.pos}: {e.msg}")
            print(f"[debug] JSON preview: {repr(json_text[:200])}")
            print(f"[debug] Error context: {repr(json_text[max(0, e.pos-20):e.pos+20])}")
            return None
    
    if not isinstance(obj, dict):
        return None

    title = obj.get("title", "")
    bullets = obj.get("bullets", [])

    if not isinstance(title, str):
        title = ""
    if not isinstance(bullets, list):
        return None

    cleaned = []
    for b in bullets:
        if not isinstance(b, dict):
            continue
        text = b.get("text", "")
        if not isinstance(text, str):
            continue

        # ä¿æŒLLMè¾“å‡ºçš„åŸå§‹æ ¼å¼ï¼Œä¸å†è¿›è¡Œå·¥ç¨‹çº§æ¸…æ´—
        text = text.strip()
        if not text:
            continue

        sub_list = b.get("sub", [])
        sub_clean = []
        if isinstance(sub_list, list):
            for s in sub_list:
                st = s.get("text", "") if isinstance(s, dict) else s
                if isinstance(st, str):
                    st = st.strip()
                    if st:
                        sub_clean.append({"text": st})

        cleaned.append({"text": text, "sub": sub_clean})

    if not cleaned:
        return None

    cleaned = cleaned[:8]
    for b in cleaned:
        b["sub"] = b["sub"][:8]

    # ä¿æŒLLMè¾“å‡ºçš„åŸå§‹æ ¼å¼ï¼Œä¸å†è¿›è¡Œå·¥ç¨‹çº§æ¸…æ´—
    return {"title": title.strip(), "bullets": cleaned}

def outline_to_markdown(outline: dict) -> str:
    lines = []
    title = (outline.get("title") or "").strip()
    if title:
        lines.append(f"**{title}**")

    for b in outline.get("bullets", []):
        lines.append(f"- {b['text']}")
        for s in b.get("sub", []):
            lines.append(f"  - {s['text']}")

    return "\n".join(lines).strip()

def normalize_markdown(text: str) -> str:
    lines = []
    for line in text.splitlines():
        line = line.rstrip()

        # ä¸¢æ‰æ˜æ˜¾çš„åºŸè¯
        if not line:
            continue
        if line.startswith(("è§£é‡Š", "è¯´æ˜", "æ³¨æ„")):
            continue

        # åªä¿ç•™ markdown åˆ—è¡¨è¡Œ
        if line.lstrip().startswith("-"):
            lines.append(line)

    return "\n".join(lines)

def smart_struct_then_render(raw_text: str) -> str:
    raw_text = (raw_text or "").strip()
    if not raw_text:
        return ""

    pre = preprocess_before_llm(raw_text)

    try:
        prompt = build_prompt_reorder(pre)  # æ³¨æ„ï¼šä¸å†æ˜¯ build_prompt_struct
        resp = call_ollama(prompt, timeout=50)

        md = normalize_markdown(resp)

        if not md.strip():
            print("[debug] struct_reorder: empty markdown output")
            return ""

        result = md.strip()
        if result:
            print("[debug] smart_struct: success, output preview:", repr(result[:200]))
        else:
            print("[debug] smart_struct: markdown conversion returned empty")
        return result
    except json.JSONDecodeError as e:
        # JSON è§£æé”™è¯¯å·²ç»åœ¨ parse_outline ä¸­å¤„ç†äº†ï¼Œè¿™é‡Œåªæ˜¯å…œåº•
        print(f"âš ï¸ smart_struct: JSON decode error at position {e.pos}: {e.msg}")
        return ""
    except Exception as e:
        print("âš ï¸ smart_struct_then_render failed:", repr(e))
        import traceback
        traceback.print_exc()
        return ""


# =========================
# 1. åˆå§‹åŒ– ASR æ¨¡å‹
# =========================
model = AutoModel(
    model="paraformer-zh-streaming",
    device="mps"
)

# =========================
# 2. æµå¼å‚æ•°
# =========================
sample_rate = 16000
chunk_size = [0, 10, 5]
encoder_chunk_look_back = 4
decoder_chunk_look_back = 1
chunk_stride = chunk_size[1] * 960

cache = {}
audio_buffer = np.zeros((0,), dtype=np.float32)
last_text = ""

text_queue = Queue()

# =========================
# 3. è¿è¡Œæ—¶çŠ¶æ€ï¼ˆpreview + commitï¼‰
# =========================
state_lock = threading.Lock()

preview_raw_text = ""
preview_len = 0
last_voice_time = time.time()
last_commit_time = 0.0
committing = False


# =========================
# 4. éŸ³é¢‘å›è°ƒï¼ˆåªè´Ÿè´£ ASR + èƒ½é‡æ£€æµ‹ + æ¨é€å¢é‡ï¼‰
# =========================
def record_callback(indata, frames, time_info, status):
    global audio_buffer, last_text, last_voice_time

    audio_mono = indata[:, 0].astype(np.float32)
    rms = float(np.sqrt(np.mean(audio_mono * audio_mono)) + 1e-12)
    if rms > ENERGY_THRESHOLD:
        last_voice_time = time.time()

    audio_buffer = np.concatenate([audio_buffer, audio_mono])

    while len(audio_buffer) >= chunk_stride:
        chunk = audio_buffer[:chunk_stride]
        audio_buffer = audio_buffer[chunk_stride:]

        res = model.generate(
            input=chunk,
            cache=cache,
            is_final=False,
            chunk_size=chunk_size,
            encoder_chunk_look_back=encoder_chunk_look_back,
            decoder_chunk_look_back=decoder_chunk_look_back,
        )

        # ä¿®å¤ç‚¹ï¼šè¿™é‡Œä¸è¦ returnï¼ˆä¼šä¸­æ–­æœ¬è½®åç»­ chunk å¤„ç†ï¼‰ï¼Œæ”¹ continue
        if not res or not res[0].get("text"):
            continue

        text = res[0]["text"]
        new_part = diff_new_part(last_text, text)

        if new_part and new_part.strip():
            text_queue.put(new_part)

        last_text = text


# =========================
# 5. Commitï¼šé™éŸ³åè§¦å‘ LLM â†’ å®‰å…¨é—¸é—¨ â†’ æ›¿æ¢ preview
# =========================
def try_commit_if_needed():
    global preview_raw_text, preview_len, last_commit_time, committing

    now = time.time()

    with state_lock:
        if committing:
            return
        if preview_len <= 0:
            return
        if now - last_voice_time < SILENCE_TIMEOUT:
            return
        if now - last_commit_time < MIN_COMMIT_GAP:
            return

        committing = True
        raw_to_process = preview_raw_text
        chars_to_delete = preview_len

    print("\nğŸ§  commit trigger -> post-process...")

    # 1ï¸âƒ£ å·¥ç¨‹é¢„æ¸…æ´—ï¼ˆåªåšå®‰å…¨ã€ç¡®å®šæ€§çš„äº‹ï¼‰
    raw_clean = preprocess_before_llm(raw_to_process)

    processed = ""

    # ===============================
    # 2ï¸âƒ£ ç»“æ„é‡æ’ä¸»è·¯å¾„ï¼ˆsmart_markdownï¼‰
    # ===============================
    if LLM_MODE == "smart_markdown":
        processed = smart_struct_then_render(raw_clean)

        if processed:
            # âš ï¸ æ³¨æ„ï¼šç»“æ„é‡æ’æ¨¡å¼ä¸‹ï¼Œåªåšâ€œåº•çº¿ guardâ€
            if not is_llm_output_safe(
                raw_clean,
                processed,
                mode="reorder"   # ğŸ‘ˆ å…³é”®ï¼šå‘Šè¯‰ guard è¿™æ˜¯é‡æ’
            ):
                print("ğŸ§¯ guard rejected reordered output -> fallback")
                processed = ""

        # ===============================
        # 3ï¸âƒ£ fallbackï¼šmarkdown â†’ clean
        # ===============================
        if not processed:
            print("[debug] smart_struct empty/rejected, trying markdown mode...")
            processed = call_ollama_postprocess(raw_clean, mode="markdown").strip()

            # markdown ä¹Ÿå¤±è´¥ï¼ˆæ²¡ç»“æ„ï¼‰
            if not processed or not any(c in processed for c in ['\n', '-', '*', '1.', '2.', '3.']):
                print("[debug] markdown mode weak, trying clean mode...")
                processed = call_ollama_postprocess(raw_clean, mode="clean").strip()

    # ===============================
    # 4ï¸âƒ£ é smart_markdown æ¨¡å¼ï¼ˆæ—§æ¨¡å¼ï¼‰
    # ===============================
    else:
        processed = call_ollama_postprocess(raw_clean, LLM_MODE).strip()

        if processed:
            if not is_llm_output_safe(raw_clean, processed, mode="format"):
                print("ğŸ§¯ guard rejected output -> keep raw_clean")
                processed = raw_clean

    # ===============================
    # 5ï¸âƒ£ æœ€ç»ˆå…œåº•
    # ===============================
    if not processed:
        processed = raw_clean if raw_clean else raw_to_process

    # ===============================
    # 6ï¸âƒ£ Debug è§‚å¯Ÿ
    # ===============================
    try:
        print("[debug] raw_clean preview:", repr(raw_clean[:200]))
        print("[debug] processed preview:", repr(processed[:200]))
    except Exception:
        pass

    # ===============================
    # 7ï¸âƒ£ æäº¤åˆ°â€œæ–‡æ¡£â€
    # ===============================
    delete_chars(chars_to_delete)
    paste_text(processed)

    with state_lock:
        preview_raw_text = ""
        preview_len = 0
        last_commit_time = time.time()
        committing = False

    print("âœ… commit done\n")

# è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°éƒ¨ç½²çš„aiè¯­éŸ³

# =========================
# 6. å¯åŠ¨éº¦å…‹é£ & ä¸»çº¿ç¨‹è¾“å‡º
# =========================
print("ğŸ™ è¯·æŠŠå…‰æ ‡æ”¾åœ¨ä»»æ„è¾“å…¥æ¡†ï¼ˆå¾®ä¿¡ / è®°äº‹æœ¬ / æµè§ˆå™¨éƒ½è¡Œï¼‰")
print("ğŸ‘‰ preview å®æ—¶å‡ºå­—ï¼Œåœé¡¿å commit ä¼šç”¨ç»“æ„åŒ–+ç¾åŒ–æ›¿æ¢ï¼ˆå¸¦å®‰å…¨é—¸é—¨ï¼‰")
print(f"ğŸ‘‰ æ¨¡å¼ï¼š{LLM_MODE} | é™éŸ³é˜ˆå€¼ï¼š{SILENCE_TIMEOUT}s | æ¨¡å‹ï¼š{OLLAMA_MODEL}")

with sd.InputStream(
    samplerate=sample_rate,
    channels=1,
    dtype="float32",
    blocksize=1024,
    callback=record_callback,
):
    try:
        while True:
            # æ›´ç¨³çš„ queue è¯»å–æ–¹å¼
            while True:
                try:
                    new_text = text_queue.get_nowait()
                except Empty:
                    break

                paste_text(new_text)

                with state_lock:
                    preview_raw_text += new_text
                    preview_len += len(new_text)

            try_commit_if_needed()
            sd.sleep(20)

    except KeyboardInterrupt:
        print("\nğŸ›‘ stopped")

# 